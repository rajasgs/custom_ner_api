{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/BIOUL_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250 entries, 0 to 249\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   tokens    250 non-null    object\n",
      " 1   ner_tags  250 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 4.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have only 250 sentences annotated which is very less but fine for small POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['Summary', '\\n\\n', 'Summary', 'Companies', 'T...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['The', 'U.S.', 'stock', 'market', 'has', 'suf...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['\"', 'The', 'U.S.', 'yield', 'markets', '(', ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['Register', 'now', 'for', 'FREE', 'unlimited'...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['All', '11', 'major', 'S&amp;P', '500', '(', '.SP...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  ['Summary', '\\n\\n', 'Summary', 'Companies', 'T...   \n",
       "1  ['The', 'U.S.', 'stock', 'market', 'has', 'suf...   \n",
       "2  ['\"', 'The', 'U.S.', 'yield', 'markets', '(', ...   \n",
       "3  ['Register', 'now', 'for', 'FREE', 'unlimited'...   \n",
       "4  ['All', '11', 'major', 'S&P', '500', '(', '.SP...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "1  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "2  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "3  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "4  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['ner_tags'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert str to list of str\n",
    "import ast\n",
    "df['ner_tags'] = df.ner_tags.apply(lambda s: list(ast.literal_eval(s)))\n",
    "df['tokens'] = df.tokens.apply(lambda s: list(ast.literal_eval(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-CUSTOM_ORG',\n",
       " 'B-CUSTOM_PERSON',\n",
       " 'B-CUSTOM_PLACE',\n",
       " 'B-CUSTOM_ROLE',\n",
       " 'I-CUSTOM_ORG',\n",
       " 'I-CUSTOM_PERSON',\n",
       " 'I-CUSTOM_PLACE',\n",
       " 'I-CUSTOM_ROLE',\n",
       " 'L-CUSTOM_ORG',\n",
       " 'L-CUSTOM_PERSON',\n",
       " 'L-CUSTOM_PLACE',\n",
       " 'L-CUSTOM_ROLE',\n",
       " 'O',\n",
       " 'U-CUSTOM_ORG',\n",
       " 'U-CUSTOM_PERSON',\n",
       " 'U-CUSTOM_PLACE',\n",
       " 'U-CUSTOM_ROLE'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "#Get list of NER tags from data collection notebook\n",
    "ner_tags = set(itertools.chain.from_iterable(df.ner_tags))\n",
    "ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe string \"-\" is used where the entity offsets don’t align with the tokenization in the Doc object. \\nThe training algorithm will view these as missing values. \\nO denotes a non-entity token. \\nB denotes the beginning of a multi-token entity, \\nI the inside of an entity of three or more tokens,\\nand L the end of an entity of two or more tokens. \\nU denotes a single-token entity.\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The string \"-\" is used where the entity offsets don’t align with the tokenization in the Doc object. \n",
    "The training algorithm will view these as missing values. \n",
    "O denotes a non-entity token. \n",
    "B denotes the beginning of a multi-token entity, \n",
    "I the inside of an entity of three or more tokens,\n",
    "and L the end of an entity of two or more tokens. \n",
    "U denotes a single-token entity.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_label = dict(enumerate(ner_tags))\n",
    "label_to_id = {v:k for k, v in id_to_label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L-CUSTOM_ROLE': 0,\n",
       " 'I-CUSTOM_ROLE': 1,\n",
       " 'I-CUSTOM_ORG': 2,\n",
       " 'U-CUSTOM_PERSON': 3,\n",
       " 'L-CUSTOM_PERSON': 4,\n",
       " 'L-CUSTOM_PLACE': 5,\n",
       " 'U-CUSTOM_PLACE': 6,\n",
       " 'I-CUSTOM_PLACE': 7,\n",
       " 'U-CUSTOM_ROLE': 8,\n",
       " 'B-CUSTOM_PERSON': 9,\n",
       " 'L-CUSTOM_ORG': 10,\n",
       " 'I-CUSTOM_PERSON': 11,\n",
       " 'U-CUSTOM_ORG': 12,\n",
       " 'B-CUSTOM_PLACE': 13,\n",
       " 'B-CUSTOM_ROLE': 14,\n",
       " 'B-CUSTOM_ORG': 15,\n",
       " 'O': 16}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id = {'B-CUSTOM_PLACE': 0,\n",
    " 'L-CUSTOM_ORG': 1,\n",
    " 'B-CUSTOM_PERSON': 2,\n",
    " 'B-CUSTOM_ROLE': 3,\n",
    " 'U-CUSTOM_PERSON': 4,\n",
    " 'I-CUSTOM_ROLE': 5,\n",
    " 'I-CUSTOM_PLACE': 6,\n",
    " 'U-CUSTOM_PLACE': 7,\n",
    " 'L-CUSTOM_ROLE': 8,\n",
    " 'I-CUSTOM_PERSON': 9,\n",
    " 'U-CUSTOM_ROLE': 10,\n",
    " 'L-CUSTOM_PERSON': 11,\n",
    " 'I-CUSTOM_ORG': 12,\n",
    " 'U-CUSTOM_ORG': 13,\n",
    " 'L-CUSTOM_PLACE': 14,\n",
    " 'B-CUSTOM_ORG': 15,\n",
    " 'O': 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'L-CUSTOM_ROLE',\n",
       " 1: 'I-CUSTOM_ROLE',\n",
       " 2: 'I-CUSTOM_ORG',\n",
       " 3: 'U-CUSTOM_PERSON',\n",
       " 4: 'L-CUSTOM_PERSON',\n",
       " 5: 'L-CUSTOM_PLACE',\n",
       " 6: 'U-CUSTOM_PLACE',\n",
       " 7: 'I-CUSTOM_PLACE',\n",
       " 8: 'U-CUSTOM_ROLE',\n",
       " 9: 'B-CUSTOM_PERSON',\n",
       " 10: 'L-CUSTOM_ORG',\n",
       " 11: 'I-CUSTOM_PERSON',\n",
       " 12: 'U-CUSTOM_ORG',\n",
       " 13: 'B-CUSTOM_PLACE',\n",
       " 14: 'B-CUSTOM_ROLE',\n",
       " 15: 'B-CUSTOM_ORG',\n",
       " 16: 'O'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_label = {0: 'B-CUSTOM_PLACE',\n",
    " 1: 'L-CUSTOM_ORG',\n",
    " 2: 'B-CUSTOM_PERSON',\n",
    " 3: 'B-CUSTOM_ROLE',\n",
    " 4: 'U-CUSTOM_PERSON',\n",
    " 5: 'I-CUSTOM_ROLE',\n",
    " 6: 'I-CUSTOM_PLACE',\n",
    " 7: 'U-CUSTOM_PLACE',\n",
    " 8: 'L-CUSTOM_ROLE',\n",
    " 9: 'I-CUSTOM_PERSON',\n",
    " 10: 'U-CUSTOM_ROLE',\n",
    " 11: 'L-CUSTOM_PERSON',\n",
    " 12: 'I-CUSTOM_ORG',\n",
    " 13: 'U-CUSTOM_ORG',\n",
    " 14: 'L-CUSTOM_PLACE',\n",
    " 15: 'B-CUSTOM_ORG',\n",
    " 16: 'O'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_fn(ls):\n",
    "    new_ls = [label_to_id[key] for key in ls]\n",
    "    return new_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ner_tags'] = df['ner_tags'].apply(replace_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 1,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 13,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['ner_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.explode('ner_tags').value_counts()\n",
    "class_counts = pd.Series(sum([item for item in df.ner_tags], [])).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict, defaultdict\n",
    "class_counts_dict = class_counts.to_dict(OrderedDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(16, 7305),\n",
       "             (2, 89),\n",
       "             (11, 89),\n",
       "             (15, 52),\n",
       "             (1, 52),\n",
       "             (7, 48),\n",
       "             (13, 46),\n",
       "             (12, 32),\n",
       "             (8, 17),\n",
       "             (3, 17),\n",
       "             (5, 10),\n",
       "             (4, 8),\n",
       "             (10, 8),\n",
       "             (0, 7),\n",
       "             (14, 7),\n",
       "             (9, 7),\n",
       "             (6, 3)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7797"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count =  class_counts.sum()\n",
    "total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7305"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "majority_sample = class_counts_dict[16]\n",
    "majority_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels are imbalanced,We can use weightsampler to oversample data but instead let's use different strategy.class_weights\n",
    "# we'll create class_weights to give more weightage to weak class during training.\n",
    "# we can use strategy for majority class , class_weight = 1-(class sample/total sample)\n",
    "# for others, class_weight = 1-(class count/total samples- majority class sample)\n",
    "class_weights = []\n",
    "for i in range(len(class_counts_dict.keys())):\n",
    "    if i==16:\n",
    "        class_weight = 1-(class_counts_dict[i]/total_count)\n",
    "    else:\n",
    "        class_weight = 1-(class_counts_dict[i]/(total_count-majority_sample))\n",
    "    class_weights.append(class_weight)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9857723577235772,\n",
       " 0.8943089430894309,\n",
       " 0.8191056910569106,\n",
       " 0.9654471544715447,\n",
       " 0.983739837398374,\n",
       " 0.9796747967479675,\n",
       " 0.9939024390243902,\n",
       " 0.9024390243902439,\n",
       " 0.9654471544715447,\n",
       " 0.9857723577235772,\n",
       " 0.983739837398374,\n",
       " 0.8191056910569106,\n",
       " 0.9349593495934959,\n",
       " 0.9065040650406504,\n",
       " 0.9857723577235772,\n",
       " 0.8943089430894309,\n",
       " 0.06310119276644865]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "#Since the input has already been split into words, set is_split_into_words=True\n",
    "tokenized_input = tokenizer(df[\"tokens\"][0], is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'summary',\n",
       " 'summary',\n",
       " 'companies',\n",
       " 'tesla',\n",
       " 'down',\n",
       " 'as',\n",
       " 'q',\n",
       " '##3',\n",
       " 'deliveries',\n",
       " 'miss',\n",
       " 'market',\n",
       " 'estimates',\n",
       " 'u',\n",
       " '.',\n",
       " 's',\n",
       " '.',\n",
       " 'factory',\n",
       " 'activity',\n",
       " 'slow',\n",
       " '##est',\n",
       " 'in',\n",
       " '~',\n",
       " '2',\n",
       " '.',\n",
       " '5',\n",
       " 'years',\n",
       " 'in',\n",
       " 'sept',\n",
       " '-',\n",
       " 'is',\n",
       " '##m',\n",
       " 'credit',\n",
       " 'sui',\n",
       " '##sse',\n",
       " ',',\n",
       " 'ci',\n",
       " '##ti',\n",
       " 'cut',\n",
       " '202',\n",
       " '##2',\n",
       " 'year',\n",
       " '-',\n",
       " 'end',\n",
       " 'target',\n",
       " 'for',\n",
       " 's',\n",
       " '&',\n",
       " 'p',\n",
       " '500',\n",
       " 'index',\n",
       " '##es',\n",
       " 'up',\n",
       " ':',\n",
       " 'dow',\n",
       " '2',\n",
       " '.',\n",
       " '66',\n",
       " '%',\n",
       " ',',\n",
       " 's',\n",
       " '&',\n",
       " 'p',\n",
       " '500',\n",
       " '2',\n",
       " '.',\n",
       " '59',\n",
       " '%',\n",
       " ',',\n",
       " 'nas',\n",
       " '##da',\n",
       " '##q',\n",
       " '2',\n",
       " '.',\n",
       " '27',\n",
       " '%',\n",
       " 'oct',\n",
       " '3',\n",
       " '(',\n",
       " 'reuters',\n",
       " ')',\n",
       " '-',\n",
       " 'wall',\n",
       " 'street',\n",
       " \"'\",\n",
       " 's',\n",
       " 'three',\n",
       " 'major',\n",
       " 'index',\n",
       " '##es',\n",
       " 'rallied',\n",
       " 'to',\n",
       " 'close',\n",
       " 'over',\n",
       " '2',\n",
       " '%',\n",
       " 'on',\n",
       " 'monday',\n",
       " 'as',\n",
       " 'u',\n",
       " '.',\n",
       " 's',\n",
       " '.',\n",
       " 'treasury',\n",
       " 'yields',\n",
       " 'tumbled',\n",
       " 'on',\n",
       " 'weaker',\n",
       " '-',\n",
       " 'than',\n",
       " '-',\n",
       " 'expected',\n",
       " 'manufacturing',\n",
       " 'data',\n",
       " ',',\n",
       " 'increasing',\n",
       " 'the',\n",
       " 'appeal',\n",
       " 'of',\n",
       " 'stocks',\n",
       " 'at',\n",
       " 'the',\n",
       " 'start',\n",
       " 'of',\n",
       " 'the',\n",
       " 'year',\n",
       " \"'\",\n",
       " 's',\n",
       " 'final',\n",
       " 'quarter',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids: {'input_ids': [101, 1045, 2572, 3449, 2239, 102], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
      "tokens: ['[CLS]', 'i', 'am', 'el', '##on', '[SEP]']\n",
      "word_ids: [None, 0, 1, 2, 2, None]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "#Since the input has already been split into words, set is_split_into_words=True\n",
    "tokenized_input = tokenizer(['I','am','Elon'], is_split_into_words=True)\n",
    "print(\"token_ids:\",tokenized_input)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(\"tokens:\",tokens)\n",
    "print(\"word_ids:\",tokenized_input.word_ids())# index of orifginal word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adding the special tokens [CLS] and [SEP] and subword tokenization creates a mismatch between the input and labels. A single word corresponding to a single label may be split into two subwords. You will need to realign the tokens and labels by:\\n\\n1.Mapping all tokens to their corresponding word with the word_ids method.\\n2.Assigning the label -100 to the special tokens [CLS] and [SEP] so the PyTorch loss function ignores them.\\n3.Only labeling the first token of a given word. Assign -100 to other subtokens from the same word.\\nhttps://huggingface.co/docs/transformers/tasks/token_classification#preprocess\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Adding the special tokens [CLS] and [SEP] and subword tokenization creates a mismatch between the input and labels. A single word corresponding to a single label may be split into two subwords. You will need to realign the tokens and labels by:\n",
    "\n",
    "1.Mapping all tokens to their corresponding word with the word_ids method.\n",
    "2.Assigning the label -100 to the special tokens [CLS] and [SEP] so the PyTorch loss function ignores them.\n",
    "3.Only labeling the first token of a given word. Assign -100 to other subtokens from the same word.\n",
    "https://huggingface.co/docs/transformers/tasks/token_classification#preprocess\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(df['tokens'])\n",
    "y = list(df['ner_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindMaxLength(lst):\n",
    "    maxList = max(lst, key = lambda i: len(i))\n",
    "    maxLength = len(maxList)\n",
    "     \n",
    "    return maxLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FindMaxLength(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have maximum tokens as 103 but Let's assume max_length as 200\n",
    "max_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y,max_length,id_to_label):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.id_to_label = id_to_label\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenize_and_align_labels(self.X[idx],self.y[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len( self.X)\n",
    "    \n",
    "    def tokenize_and_align_labels(self,x_el,y_el):\n",
    "        #print(row[\"tokens\"])\n",
    "        #print( row['ner_tags'])\n",
    "        tokenized_inputs = self.tokenizer(x_el, truncation=True,padding=\"max_length\", is_split_into_words=True,max_length=max_length)\n",
    "        data = {key: torch.tensor(val) for key, val in tokenized_inputs.items()}\n",
    "        #print(tokenized_inputs)\n",
    "        ner_tags = y_el\n",
    "        labels = []\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=0)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(ner_tags[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "        data[\"labels\"] = torch.tensor(labels).squeeze()\n",
    "        return data\n",
    "\n",
    "train_dataset = Dataset(X, y,max_length,id_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101, 12654, 12654,  3316, 26060,  2091,  2004,  1053,  2509, 23534,\n",
      "         3335,  3006, 10035,  1057,  1012,  1055,  1012,  4713,  4023,  4030,\n",
      "         4355,  1999,  1066,  1016,  1012,  1019,  2086,  1999, 17419,  1011,\n",
      "         2003,  2213,  4923, 24086, 11393,  1010, 25022,  3775,  3013, 16798,\n",
      "         2475,  2095,  1011,  2203,  4539,  2005,  1055,  1004,  1052,  3156,\n",
      "         5950,  2229,  2039,  1024, 23268,  1016,  1012,  5764,  1003,  1010,\n",
      "         1055,  1004,  1052,  3156,  1016,  1012,  5354,  1003,  1010, 17235,\n",
      "         2850,  4160,  1016,  1012,  2676,  1003, 13323,  1017,  1006, 26665,\n",
      "         1007,  1011,  2813,  2395,  1005,  1055,  2093,  2350,  5950,  2229,\n",
      "        24356,  2000,  2485,  2058,  1016,  1003,  2006,  6928,  2004,  1057,\n",
      "         1012,  1055,  1012,  9837, 16189, 18303,  2006, 15863,  1011,  2084,\n",
      "         1011,  3517,  5814,  2951,  1010,  4852,  1996,  5574,  1997, 15768,\n",
      "         2012,  1996,  2707,  1997,  1996,  2095,  1005,  1055,  2345,  4284,\n",
      "         1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "tensor([-100,   16,   16,   16,   16,   16,   16,   16, -100,   16,   16,   16,\n",
      "          16,   16, -100, -100, -100,   16,   16,   16, -100,   16,   16, -100,\n",
      "        -100, -100,   16,   16,   16,   16, -100, -100,   15,    1, -100,   16,\n",
      "          16, -100,   16,   16, -100,   16,   16,   16,   16,   16,   16, -100,\n",
      "        -100,   16,   16, -100,   16,   16,   16,   16, -100, -100,   16,   16,\n",
      "          16, -100, -100,   16,   16, -100, -100,   16,   16,   16, -100, -100,\n",
      "          16, -100, -100,   16,   16,   16,   16,   13,   16,   16,   16,   16,\n",
      "          16, -100,   16,   16,   16, -100,   16,   16,   16,   16,   16,   16,\n",
      "          16,   16,   16,   16, -100, -100, -100,   16,   16,   16,   16,   16,\n",
      "          16,   16,   16,   16,   16,   16,   16,   16,   16,   16,   16,   16,\n",
      "          16,   16,   16,   16,   16,   16,   16, -100,   16,   16,   16, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100])\n"
     ]
    }
   ],
   "source": [
    "for data in train_dataset:\n",
    "    print(data['input_ids'])\n",
    "    print(data['labels'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Custom Model training\\nfrom torch.utils.data import Dataset, TensorDataset,DataLoader\\ntrain_loader = DataLoader(train_dataset,batch_size=16, shuffle=True)\\n\\nNUM_EPOCHS = 1\\nLEARNING_RATE = 0.01\\noptimizer =torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE) \\nfor i in range(NUM_EPOCHS):\\n  model.train()\\n  for X_batch,y_batch in train_loader:\\n    output = model(X_batch,labels=y_batch)\\n    output.loss.backward()\\n    optimizer.step()\\n    optimizer.zero_grad()'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Custom Model training\n",
    "from torch.utils.data import Dataset, TensorDataset,DataLoader\n",
    "train_loader = DataLoader(train_dataset,batch_size=16, shuffle=True)\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 0.01\n",
    "optimizer =torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE) \n",
    "for i in range(NUM_EPOCHS):\n",
    "  model.train()\n",
    "  for X_batch,y_batch in train_loader:\n",
    "    output = model(X_batch,labels=y_batch)\n",
    "    output.loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    num_train_epochs=1,\n",
    "    report_to =None\n",
    "\n",
    ")\n",
    "\n",
    "#We should override comput_loss to inform trainerr about class imbalance(we have majority as \"O\" i.e not an entity)\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 17 labels with different weights)\n",
    "        #class_weights = torch.tensor([3.0, 3.0, 3.0,3.0, 3.0, 3.0,3.0, 3.0, 3.0,3.0, 3.0, 3.0,3.0, 3.0, 3.0,3.0, 0.2])\n",
    "        class_weights = torch.tensor([0.9857723577235772,0.8943089430894309, 0.8191056910569106, 0.9654471544715447, 0.983739837398374,\n",
    "                        0.9796747967479675, 0.9939024390243902, 0.9024390243902439, 0.9654471544715447, 0.9857723577235772, 0.983739837398374, 0.8191056910569106,\n",
    "                        0.9349593495934959, 0.9065040650406504, 0.9857723577235772, 0.8943089430894309, 0.06310119276644865])\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights,reduction='mean')\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\metes\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 250\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 32\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04ee40439ba4639b50a6c286c3a06b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 120.7696, 'train_samples_per_second': 2.07, 'train_steps_per_second': 0.265, 'train_loss': 1.5432997941970825, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=32, training_loss=1.5432997941970825, metrics={'train_runtime': 120.7696, 'train_samples_per_second': 2.07, 'train_steps_per_second': 0.265, 'train_loss': 1.5432997941970825, 'epoch': 1.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../models/custom_ner_transformers\n",
      "Configuration saved in ../models/custom_ner_transformers\\config.json\n",
      "Model weights saved in ../models/custom_ner_transformers\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"../models/custom_ner_transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\metes/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\metes/.cache\\huggingface\\transformers\\0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\metes/.cache\\huggingface\\transformers\\75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\metes/.cache\\huggingface\\transformers\\8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\metes/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file ../models/custom_ner_transformers\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"../models/custom_ner_transformers\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ../models/custom_ner_transformers\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForTokenClassification.\n",
      "\n",
      "All the weights of DistilBertForTokenClassification were initialized from the model checkpoint at ../models/custom_ner_transformers.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#Inference Code\n",
    "from transformers import AutoModelForTokenClassification,AutoTokenizer\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "saved_model = AutoModelForTokenClassification.from_pretrained(\"../models/custom_ner_transformers\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference Code\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "#text = 'The Tesla factory is seen in Fremont, California, U.S'\n",
    "text = 'My name is Elon Musk and I am CEO of Tesla'\n",
    "spacy_tokens = nlp(text)\n",
    "text_tokens = [token.text for token in spacy_tokens]\n",
    "tokenized_inputs = tokenizer(text_tokens,truncation=True,padding=\"max_length\", is_split_into_words=True,max_length=200, return_tensors=\"pt\")\n",
    "outputs = saved_model(**tokenized_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200, 17])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.shape\n",
    "#batch_size*seq_length*num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3212,  0.0098,  0.1914,  0.0162, -0.3617, -0.2911, -0.3385,  0.2026,\n",
       "        -0.3815, -0.8662, -0.7390,  0.2112, -0.6786, -0.2968, -0.7633,  0.1688,\n",
       "         2.2246], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.squeeze()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(outputs.logits.squeeze()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200])\n",
      "['O', 'O', 'O', 'O', 'B-CUSTOM_PERSON', 'B-CUSTOM_PERSON', 'L-CUSTOM_PERSON', 'L-CUSTOM_PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CUSTOM_PERSON', 'O', 'O', 'O', 'O', 'B-CUSTOM_PERSON', 'L-CUSTOM_PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CUSTOM_PERSON', 'L-CUSTOM_PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CUSTOM_PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CUSTOM_PERSON', 'O', 'O', 'B-CUSTOM_PERSON', 'B-CUSTOM_PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CUSTOM_PERSON', 'L-CUSTOM_PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CUSTOM_PERSON', 'O', 'O', 'B-CUSTOM_PERSON', 'B-CUSTOM_PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CUSTOM_PERSON', 'L-CUSTOM_PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['[CLS]', 'my', 'name', 'is', 'el', '##on', 'mu', '##sk', 'and', 'i', 'am', 'ceo', 'of', 'tesla', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "word_ids: [None, 0, 1, 2, 3, 3, 4, 4, 5, 6, 7, 8, 9, 10, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "words: ['My', 'name', 'is', 'Elon', 'Musk', 'and', 'I', 'am', 'CEO', 'of', 'Tesla']\n"
     ]
    }
   ],
   "source": [
    "predictions = torch.argmax(outputs.logits.squeeze(), axis=1)\n",
    "print(predictions.shape)\n",
    "predictions = [id_to_label[int(i)] for i in predictions]\n",
    "print(predictions)\n",
    "words = tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"][0])\n",
    "print(words)\n",
    "word_ids = tokenized_inputs.word_ids()\n",
    "print(\"word_ids:\",word_ids)# index of original word\n",
    "print(\"words:\",text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 My\n",
      "1 name\n",
      "2 is\n",
      "3 Elon\n",
      "4 Musk\n",
      "5 and\n",
      "6 I\n",
      "7 am\n",
      "8 CEO\n",
      "9 of\n",
      "10 Tesla\n"
     ]
    }
   ],
   "source": [
    "token_entity = {}\n",
    "for idx,word in enumerate(text_tokens):\n",
    "    print(idx,word)\n",
    "    for id,word_id in enumerate(word_ids):\n",
    "        if idx==word_id:\n",
    "            if word not in token_entity:\n",
    "                token_entity[word] = predictions[id]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'My': 'O',\n",
       " 'name': 'O',\n",
       " 'is': 'O',\n",
       " 'Elon': 'B-CUSTOM_PERSON',\n",
       " 'Musk': 'L-CUSTOM_PERSON',\n",
       " 'and': 'O',\n",
       " 'I': 'O',\n",
       " 'am': 'O',\n",
       " 'CEO': 'O',\n",
       " 'of': 'O',\n",
       " 'Tesla': 'O'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My 0 2\n",
      "name 3 7\n",
      "is 8 10\n",
      "Elon 11 15\n",
      "Musk 16 20\n",
      "and 21 24\n",
      "I 25 26\n",
      "am 27 29\n",
      "CEO 30 33\n",
      "of 34 36\n",
      "Tesla 37 42\n"
     ]
    }
   ],
   "source": [
    "for token in spacy_tokens:\n",
    "  print(token.text, token.idx, token.idx + len(token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for token in spacy_tokens:\n",
    "    if token_entity[token.text] !='O':\n",
    "        output.append(\n",
    "                {\n",
    "                    \"start\": token.idx,\n",
    "                    \"end\": token.idx + len(token.text),\n",
    "                    \"label\":token_entity[token.text],\n",
    "                }\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 11, 'end': 15, 'label': 'B-CUSTOM_PERSON'},\n",
       " {'start': 16, 'end': 20, 'label': 'L-CUSTOM_PERSON'}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">My name is \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Elon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-CUSTOM_PERSON</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Musk\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">L-CUSTOM_PERSON</span>\n",
       "</mark>\n",
       " and I am CEO of Tesla</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ex = [{\"text\": text, \"ents\": output}]\n",
    "spacy.displacy.render(\n",
    "                ex, style=\"ent\", manual=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('custom_ner_api')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65b0890c2d2158dcd8be9f47177c3a42c18c52e318c083338fe94160c27e23cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
