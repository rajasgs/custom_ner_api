{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert labeled data to BILOU format\n",
    "### Create dataset with one word per row\n",
    "\n",
    "#BILOU format\n",
    "#single word,entity  once we encode/tokenize the word we get sequence of tokens instead of 1 token because BERT will add extra   \n",
    "#tokens like sub word,CLS,SEP   \n",
    "#Can we treat this sequence of tokens of a single word as sequence classification for NER?   \n",
    "#I've explored this in this notebook.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../../data/paragraphs/10_paragraph_clean_data_annotated.jsonl', 'r') as file:\n",
    "    labeled_data =  [json.loads(line) for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 2970,\n",
       " 'text': 'Summary Summary Companies Tesla down as Q3 deliveries miss market estimates U S factory activity slowest in 2 5 years in Sept ISM Credit Suisse Citi cut 2022 year end target for S P 500 Indexes up Dow 2 66 S P 500 2 59 Nasdaq 2 27 Oct 3 Reuters Wall Street is three major indexes rallied to close over 2 on Monday as U S Treasury yields tumbled on weaker than expected manufacturing data increasing the appeal of stocks at the start of the year is final quarter.The U S stock market has suffered three quarterly declines in a row in a tumultuous year marked by interest rate hikes to tame historically high inflation and concerns about a slowing economy.The U S yield markets are pulling back that is been a positive and that connotes a more risk on environment said Art Hogan chief market strategist at B Riley Wealth in Boston.Register now for FREE unlimited access to Reuters com Register Further supporting rate sensitive growth stocks the benchmark U S 10 year Treasury yield fell after British Prime Minister Liz Truss was forced to reverse course on a tax cut for the highest rate.All 11 major S P 500 SPX sectors advanced to positive territory with energy SPNY being the biggest gainer.Oil majors Exxon Mobil Corp XOM N and Chevron Corp rose more than 5 tracking a jump in crude prices as sources said the Organization of the Petroleum Exporting Countries and its allies are considering their biggest output cut since the start of the COVID 19 pandemic.Megacap growth and technology companies such as Apple Inc AAPL O and Microsoft Corp MSFT O rose over 3 respectively while banks SPXBK advanced 3.Data showed manufacturing activity increased at its slowest pace in nearly 2 1 2 years in September as new orders contracted likely as rising interest rates to tame inflation cooled demand for goods.read more The Institute for Supply Management said its manufacturing PMI dropped to 50 9 this month missing estimates but still above 50 indicating growth.The economic data stream actually came in worse than expected.In a very counterintuitive fashion that likely represents good news for equity markets said Hogan.Traders work on the floor of the New York Stock Exchange NYSE in New York City U S September 26 2022.REUTERS Brendan McDermid While good economic data strong readings had been a catalyst for selling this is the first time we have actually seen some negative news be a catalyst.All three major indexes ended a volatile third quarter lower on Friday on growing fears that the Federal Reserve is aggressive monetary policy will tip the economy into recession.The Dow Jones Industrial Average DJI rose 765 38 points or 2 66 to 29 490 89 the S P 500 SPX gained 92 81 points or 2 59 at 3 678 43 and the Nasdaq Composite IXIC added 239 82 points or 2 27 at 10 815 44.Volume on U S exchanges was 11 61 billion shares compared with the 11 54 billion average for the full session over the last 20 trading days.Tesla Inc TSLA O fell 8 6 after it sold fewer than expected vehicles in the third quarter as deliveries lagged way behind production due to logistic hurdles.Peers Lucid Group LCID O gained 0 9 and Rivian Automotive RIVN O fell 3 1.read more Major automakers are expected to report modest declines in U S new vehicle sales but analysts and investors worry that a darkening economic picture not inventory shortages will lead to weaker car sales.read more Citigroup and Credit Suisse became the latest brokerages to lower 2022 year end targets for the S P 500 as U S equity markets bear the heat of aggressive central bank actions to tamp down inflation.read more Credit Suisse also set a 2023 year end price target for the benchmark index at 4 050 points adding that 2023 would be a year of weak non recessionary growth and falling inflation.Advancing issues outnumbered decliners on the NYSE by a 5 04 to 1 ratio on Nasdaq a 2 70 to 1 ratio favored advancers.The S P 500 posted one new 52 week high and 23 new lows the Nasdaq Composite recorded 58 new highs and 282 new lows.Register now for FREE unlimited access to Reuters com Register Reporting by Echo Wang in New York Additional reporting by Ankika Biswas and Bansari Mayur Kamdar in Bengaluru Editing by Anil D Silva Arun Koyyur and Richard Chang Our Standards The Thomson Reuters Trust Principles',\n",
       " 'label': [[26, 32, 'ORG_CUSTOM'],\n",
       "  [108, 117, 'TIME_CUSTOM'],\n",
       "  [219, 226, 'ORG_CUSTOM'],\n",
       "  [245, 256, 'PLACE_CUSTOM'],\n",
       "  [307, 314, 'TIME_CUSTOM'],\n",
       "  [466, 469, 'PLACE_CUSTOM'],\n",
       "  [658, 661, 'PLACE_CUSTOM'],\n",
       "  [767, 776, 'PERSON_CUSTOM'],\n",
       "  [804, 818, 'ORG_CUSTOM'],\n",
       "  [822, 828, 'ORG_CUSTOM'],\n",
       "  [871, 879, 'ORG_CUSTOM'],\n",
       "  [1015, 1024, 'PERSON_CUSTOM'],\n",
       "  [1205, 1221, 'ORG_CUSTOM'],\n",
       "  [1509, 1518, 'ORG_CUSTOM'],\n",
       "  [1530, 1544, 'ORG_CUSTOM'],\n",
       "  [1696, 1706, 'TIME_CUSTOM'],\n",
       "  [2153, 2181, 'ORG_CUSTOM'],\n",
       "  [2185, 2198, 'PLACE_CUSTOM'],\n",
       "  [2199, 2202, 'PLACE_CUSTOM'],\n",
       "  [2203, 2220, 'TIME_CUSTOM'],\n",
       "  [2221, 2229, 'ORG_CUSTOM'],\n",
       "  [2229, 2245, 'PERSON_CUSTOM'],\n",
       "  [2461, 2467, 'TIME_CUSTOM'],\n",
       "  [2494, 2509, 'ORG_CUSTOM'],\n",
       "  [2580, 2589, 'ORG_CUSTOM'],\n",
       "  [2920, 2929, 'ORG_CUSTOM'],\n",
       "  [3077, 3094, 'ORG_CUSTOM'],\n",
       "  [3117, 3134, 'ORG_CUSTOM'],\n",
       "  [3373, 3382, 'ORG_CUSTOM'],\n",
       "  [3387, 3400, 'ORG_CUSTOM'],\n",
       "  [3439, 3452, 'TIME_CUSTOM'],\n",
       "  [3480, 3483, 'PLACE_CUSTOM'],\n",
       "  [3581, 3594, 'ORG_CUSTOM'],\n",
       "  [3605, 3619, 'TIME_CUSTOM'],\n",
       "  [3835, 3842, 'ORG_CUSTOM'],\n",
       "  [4070, 4079, 'PERSON_CUSTOM'],\n",
       "  [4083, 4091, 'PLACE_CUSTOM'],\n",
       "  [4116, 4129, 'PERSON_CUSTOM'],\n",
       "  [4134, 4154, 'PERSON_CUSTOM'],\n",
       "  [4158, 4168, 'PLACE_CUSTOM'],\n",
       "  [4179, 4191, 'PERSON_CUSTOM'],\n",
       "  [4192, 4203, 'PERSON_CUSTOM'],\n",
       "  [4208, 4222, 'PERSON_CUSTOM'],\n",
       "  [4236, 4247, 'ORG_CUSTOM'],\n",
       "  [4248, 4256, 'ORG_CUSTOM']]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert this data to BILOU format using Spacy\n",
    "import spacy\n",
    "from spacy.training import offsets_to_biluo_tags\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\metes\\Anaconda3\\envs\\custom_ner_api\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"Summary Summary Companies Tesla down as Q3 deliver...\" with entities \"[[26, 32, 'ORG_CUSTOM'], [108, 117, 'TIME_CUSTOM']...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\metes\\Anaconda3\\envs\\custom_ner_api\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"Business Logistics issues see Tesla shares slide P...\" with entities \"[[30, 36, 'ORG_CUSTOM'], [56, 62, 'ORG_CUSTOM'], [...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\metes\\Anaconda3\\envs\\custom_ner_api\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"SAN FRANCISCO Oct 4 Reuters Tesla Inc TSLA O said ...\" with entities \"[[0, 13, 'PLACE_CUSTOM'], [14, 19, 'TIME_CUSTOM'],...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\metes\\Anaconda3\\envs\\custom_ner_api\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"Business Demand for Tesla cars could be cooling of...\" with entities \"[[20, 26, 'ORG_CUSTOM'], [79, 85, 'ORG_CUSTOM'], [...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\metes\\Anaconda3\\envs\\custom_ner_api\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"Business Tesla CEO Elon Musk showcases humanoid ro...\" with entities \"[[9, 14, 'ORG_CUSTOM'], [19, 28, 'PERSON_CUSTOM'],...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\metes\\Anaconda3\\envs\\custom_ner_api\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"Business Tesla to keep Shanghai plant below maximu...\" with entities \"[[9, 14, 'ORG_CUSTOM'], [23, 31, 'PLACE_CUSTOM'], ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\metes\\Anaconda3\\envs\\custom_ner_api\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"Business Pepsi set to get first Tesla electric tru...\" with entities \"[[9, 15, 'ORG_CUSTOM'], [32, 38, 'ORG_CUSTOM'], [6...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\metes\\Anaconda3\\envs\\custom_ner_api\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"Summary Summary Law firms Arbitrator should hear c...\" with entities \"[[91, 97, 'ORG_CUSTOM'], [189, 196, 'ORG_CUSTOM'],...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\metes\\Anaconda3\\envs\\custom_ner_api\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"Model Y cars are pictured during the opening cerem...\" with entities \"[[65, 71, 'ORG_CUSTOM'], [104, 114, 'PLACE_CUSTOM'...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\metes\\Anaconda3\\envs\\custom_ner_api\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"SHANGHAI Sept 27 Reuters Tesla TSLA O plans to hol...\" with entities \"[[0, 9, 'PLACE_CUSTOM'], [9, 16, 'TIME_CUSTOM'], [...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "entities = []\n",
    "final_data = pd.DataFrame(columns = ['word','ner_tag'])\n",
    "for row in labeled_data:\n",
    "    doc = nlp(row['text'])\n",
    "    entities = row['label']\n",
    "     \n",
    "    iob_tags = offsets_to_biluo_tags(doc, entities)\n",
    "    for i in range(len(iob_tags)):\n",
    "        new_row = pd.Series({'word': doc[i].text, 'ner_tag': iob_tags[i]}) \n",
    "        \n",
    "        final_data = pd.concat([final_data, new_row.to_frame().T], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3613 entries, 0 to 3612\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   word     3613 non-null   object\n",
      " 1   ner_tag  3613 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 56.6+ KB\n"
     ]
    }
   ],
   "source": [
    "final_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe string \"-\" is used where the entity offsets don’t align with the tokenization in the Doc object. \\nThe training algorithm will view these as missing values. \\nO denotes a non-entity token. \\nB denotes the beginning of a multi-token entity, \\nI the inside of an entity of three or more tokens,\\nand L the end of an entity of two or more tokens. \\nU denotes a single-token entity.\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.ner_tag.unique()\n",
    "'''\n",
    "The string \"-\" is used where the entity offsets don’t align with the tokenization in the Doc object. \n",
    "The training algorithm will view these as missing values. \n",
    "O denotes a non-entity token. \n",
    "B denotes the beginning of a multi-token entity, \n",
    "I the inside of an entity of three or more tokens,\n",
    "and L the end of an entity of two or more tokens. \n",
    "U denotes a single-token entity.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can consider - as non entity.\n",
    "final_data['ner_tag'].replace(['-'],\n",
    "                        [\"O\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv('../../data/paragraphs/BIOUL_format_data_words.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/paragraphs/BIOUL_format_data_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3613 entries, 0 to 3612\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   word     3613 non-null   object\n",
      " 1   ner_tag  3613 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 56.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'word':'text', 'ner_tag':'label'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3613 entries, 0 to 3612\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    3613 non-null   object\n",
      " 1   label   3613 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 56.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O                  3378\n",
       "B-PERSON_CUSTOM      36\n",
       "L-PERSON_CUSTOM      36\n",
       "L-ORG_CUSTOM         24\n",
       "B-ORG_CUSTOM         24\n",
       "U-ORG_CUSTOM         20\n",
       "I-ORG_CUSTOM         17\n",
       "B-TIME_CUSTOM        14\n",
       "L-TIME_CUSTOM        14\n",
       "L-PLACE_CUSTOM       12\n",
       "B-PLACE_CUSTOM       12\n",
       "U-TIME_CUSTOM         7\n",
       "U-PLACE_CUSTOM        6\n",
       "I-PERSON_CUSTOM       5\n",
       "I-TIME_CUSTOM         4\n",
       "I-PLACE_CUSTOM        1\n",
       "B-MONEY_CUSTOM        1\n",
       "I-MONEY_CUSTOM        1\n",
       "L-MONEY_CUSTOM        1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing values\n",
    "df['label'].replace(['O', 'B-TIME_CUSTOM', 'I-TIME_CUSTOM', 'L-TIME_CUSTOM','U-TIME_CUSTOM',\n",
    "       'B-PLACE_CUSTOM','I-PLACE_CUSTOM', 'L-PLACE_CUSTOM', 'U-PLACE_CUSTOM',\n",
    "       'B-PERSON_CUSTOM','I-PERSON_CUSTOM', 'L-PERSON_CUSTOM', 'U-PERSON_CUSTOM',\n",
    "       'B-ORG_CUSTOM','I-ORG_CUSTOM', 'L-ORG_CUSTOM', 'U-ORG_CUSTOM',\n",
    "       'B-MONEY_CUSTOM','I-MONEY_CUSTOM', 'L-MONEY_CUSTOM', 'U-MONEY_CUSTOM'],\n",
    "                        [0, 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     3378\n",
       "9       36\n",
       "11      36\n",
       "15      24\n",
       "13      24\n",
       "16      20\n",
       "14      17\n",
       "1       14\n",
       "3       14\n",
       "7       12\n",
       "5       12\n",
       "4        7\n",
       "8        6\n",
       "10       5\n",
       "2        4\n",
       "6        1\n",
       "17       1\n",
       "18       1\n",
       "19       1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tags = ['O', 'B-TIME_CUSTOM', 'I-TIME_CUSTOM', 'L-TIME_CUSTOM','U-TIME_CUSTOM',\n",
    "       'B-PLACE_CUSTOM','I-PLACE_CUSTOM', 'L-PLACE_CUSTOM', 'U-PLACE_CUSTOM',\n",
    "       'B-PERSON_CUSTOM','I-PERSON_CUSTOM', 'L-PERSON_CUSTOM', 'U-PERSON_CUSTOM',\n",
    "       'B-ORG_CUSTOM','I-ORG_CUSTOM', 'L-ORG_CUSTOM', 'U-ORG_CUSTOM',\n",
    "       'B-MONEY_CUSTOM','I-MONEY_CUSTOM', 'L-MONEY_CUSTOM', 'U-MONEY_CUSTOM']\n",
    "id_to_label = dict(enumerate(ner_tags))\n",
    "label_to_id = {v:k for k, v in id_to_label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=21, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "#The AutoTokenizer.from_pretrained method takes in the name of the model to build the appropriate tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "#tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "#We can  ask the model to return all hidden states and all attention weights if we need them:output_hidden_states=True, output_attentions=True\n",
    "#But in this we don't need them\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased',num_labels=21)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization: This will return input ids:list of numbers,these numberrs are fetched from pretrained vocab\n",
    "tokenized_text =tokenizer(list(df[\"text\"]),padding=True,truncation=True,max_length=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(tokenized_text['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3613, 7])\n",
      "tensor([  101, 12654,   102,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "#Dataset preparation\n",
    "from torch.utils.data import Dataset, TensorDataset,DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "X = input_ids\n",
    "print(X.shape)\n",
    "print(X[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = torch.from_numpy(df['label'].values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,stratify=y)\n",
    "train_data = TensorDataset(X, y)\n",
    "train_loader = DataLoader(train_data,batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 0.01\n",
    "optimizer =torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE) \n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "for i in range(NUM_EPOCHS):\n",
    "  model.train()\n",
    "  for X_batch,y_batch in train_loader:\n",
    "    output = model(X_batch,labels=y_batch)\n",
    "    loss = loss_fn(output.logits,y_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../models/custom_ner_dl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = AutoModelForSequenceClassification.from_pretrained(\"../models/custom_ner_dl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 2637,  102]])\n",
      "tensor([[  3.3842,  -3.9077,  -9.7438,  -3.7688,  -4.7408,  -5.5589, -12.0809,\n",
      "          -5.6331,  -7.7037,  -3.2073,  -8.1401,  -2.2882, -14.0165,  -3.5293,\n",
      "          -3.2150,  -3.1409,  -3.4714, -10.1137, -12.3099, -11.5177, -14.1804]])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\metes\\AppData\\Local\\Temp\\ipykernel_27540\\428674701.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(tokenized_text['input_ids']).to(device)\n"
     ]
    }
   ],
   "source": [
    "#Inference Code\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "tokenized_text =tokenizer([\"America\"],padding=True,truncation=True,max_length=512,return_tensors='pt').to(device)\n",
    "input_ids = torch.tensor(tokenized_text['input_ids']).to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = saved_model(input_ids)\n",
    "\n",
    "print(input_ids)\n",
    "print(outputs.logits)\n",
    "predicted_class_id = output.logits.argmax(dim= -1)\n",
    "print(predicted_class_id)\n",
    "print(id_to_label[predicted_class_id[0].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('custom_ner_api')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65b0890c2d2158dcd8be9f47177c3a42c18c52e318c083338fe94160c27e23cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
